{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization for LLM Optimization\n",
    "\n",
    "This notebook demonstrates how to apply quantization techniques to further optimize our distilled student model. Quantization reduces the precision of model weights, enabling significant model size reduction with minimal impact on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our modules\n",
    "from src.models.distilbert import load_distilbert_model, get_device\n",
    "from src.models.knowledge_distillation import create_student_model\n",
    "from src.models.quantization import apply_dynamic_quantization, convert_to_fp16, export_to_onnx, QuantizedModelWrapper\n",
    "from src.data.dataset import load_and_prepare_data, prepare_batch_for_model\n",
    "from src.utils.metrics import measure_performance, save_metrics, print_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_LABELS = 2  # Binary classification\n",
    "\n",
    "# Dataset configuration\n",
    "DATASET_NAME = \"glue\"\n",
    "DATASET_CONFIG = \"sst2\"  # Stanford Sentiment Treebank\n",
    "BATCH_SIZE = 16\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = Path(\"../outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "STUDENT_DIR = OUTPUT_DIR / \"distilled_model\"\n",
    "QUANTIZED_DIR = OUTPUT_DIR / \"quantized_model\"\n",
    "QUANTIZED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data\n",
    "\n",
    "Load the baseline teacher model and our distilled student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics for comparison\n",
    "print(\"Loading baseline metrics...\")\n",
    "try:\n",
    "    baseline_metrics = torch.load(OUTPUT_DIR / \"baseline_metrics.pt\")\n",
    "    student_metrics = torch.load(OUTPUT_DIR / \"student_metrics.pt\")\n",
    "    print(\"Loaded metrics successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Metrics files not found. Run baseline_measurement.ipynb and knowledge_distillation.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model (for comparison)\n",
    "print(f\"Loading teacher model {MODEL_NAME}...\")\n",
    "teacher_model, tokenizer = load_distilbert_model(MODEL_NAME, NUM_LABELS)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "tokenizer, train_dataloader, eval_dataloader = load_and_prepare_data(\n",
    "    tokenizer, \n",
    "    dataset_name=DATASET_NAME, \n",
    "    dataset_config=DATASET_CONFIG,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    max_length=MAX_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student model with the same architecture as used in knowledge distillation\n",
    "print(\"Creating student model...\")\n",
    "student_model = create_student_model(\n",
    "    teacher_model=teacher_model,\n",
    "    num_layers=2,  # Using 2 layers as was done in knowledge distillation\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# Load student model weights if available\n",
    "if STUDENT_DIR.exists():\n",
    "    print(f\"Loading student model weights from {STUDENT_DIR}...\")\n",
    "    # Load student model from saved files\n",
    "    from transformers import DistilBertForSequenceClassification\n",
    "    student_model = DistilBertForSequenceClassification.from_pretrained(STUDENT_DIR)\n",
    "else:\n",
    "    print(\"Warning: Student model directory not found. Run knowledge_distillation.ipynb first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization Techniques\n",
    "\n",
    "Apply different quantization techniques to the student model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 FP16 Quantization (Half Precision)\n",
    "\n",
    "Convert floating-point numbers from 32-bit to 16-bit precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FP16 quantization\n",
    "print(\"Applying FP16 quantization...\")\n",
    "fp16_model, fp16_size_before, fp16_size_after = convert_to_fp16(student_model)\n",
    "\n",
    "# Calculate size reduction\n",
    "fp16_reduction = (fp16_size_before - fp16_size_after) / fp16_size_before * 100\n",
    "print(f\"FP16 quantization results:\")\n",
    "print(f\"  Size before: {fp16_size_before:.2f} MB\")\n",
    "print(f\"  Size after: {fp16_size_after:.2f} MB\")\n",
    "print(f\"  Reduction: {fp16_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 INT8 Dynamic Quantization\n",
    "\n",
    "Apply dynamic quantization to convert weights to 8-bit integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "print(\"Applying dynamic INT8 quantization...\")\n",
    "try:\n",
    "    int8_model, int8_size_before, int8_size_after = apply_dynamic_quantization(student_model, torch.qint8)\n",
    "    \n",
    "    # Calculate size reduction\n",
    "    int8_reduction = (int8_size_before - int8_size_after) / int8_size_before * 100\n",
    "    print(f\"INT8 quantization results:\")\n",
    "    print(f\"  Size before: {int8_size_before:.2f} MB\")\n",
    "    print(f\"  Size after: {int8_size_after:.2f} MB\")\n",
    "    print(f\"  Reduction: {int8_reduction:.2f}%\")\n",
    "    \n",
    "    # Wrap the quantized model\n",
    "    int8_model = QuantizedModelWrapper(int8_model)\n",
    "except Exception as e:\n",
    "    print(f\"INT8 quantization failed: {e}\")\n",
    "    int8_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Quantized Models\n",
    "\n",
    "Measure performance of the quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to prepare batch\n",
    "def prepare_batch(batch, device):\n",
    "    return prepare_batch_for_model(batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FP16 model\n",
    "print(\"Evaluating FP16 quantized model...\")\n",
    "fp16_metrics = measure_performance(fp16_model, eval_dataloader, DEVICE, prepare_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate INT8 model (if available)\n",
    "if int8_model is not None:\n",
    "    print(\"Evaluating INT8 quantized model...\")\n",
    "    try:\n",
    "        int8_metrics = measure_performance(int8_model, eval_dataloader, 'cpu', prepare_batch)  # Use CPU for INT8\n",
    "    except Exception as e:\n",
    "        print(f\"INT8 evaluation failed: {e}\")\n",
    "        int8_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metrics\n",
    "save_metrics(fp16_metrics, file_path=OUTPUT_DIR / \"fp16_metrics.pt\")\n",
    "if int8_metrics is not None:\n",
    "    save_metrics(int8_metrics, file_path=OUTPUT_DIR / \"int8_metrics.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Performance Across All Models\n",
    "\n",
    "Compare the performance of the original, distilled, and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics for all models\n",
    "print(\"\\n===== Teacher Model (Baseline) =====\")\n",
    "print_metrics(\n",
    "    baseline_metrics, \n",
    "    model_name=MODEL_NAME, \n",
    "    dataset_info=f\"Text Classification - {DATASET_NAME}/{DATASET_CONFIG}\"\n",
    ")\n",
    "\n",
    "print(\"\\n===== Student Model (Distilled) =====\")\n",
    "print_metrics(\n",
    "    student_metrics, \n",
    "    model_name=f\"Distilled {MODEL_NAME} (2 layers)\", \n",
    "    dataset_info=f\"Text Classification - {DATASET_NAME}/{DATASET_CONFIG}\"\n",
    ")\n",
    "\n",
    "print(\"\\n===== FP16 Model (Quantized) =====\")\n",
    "print_metrics(\n",
    "    fp16_metrics, \n",
    "    model_name=f\"FP16 Quantized Student\", \n",
    "    dataset_info=f\"Text Classification - {DATASET_NAME}/{DATASET_CONFIG}\"\n",
    ")\n",
    "\n",
    "if int8_metrics is not None:\n",
    "    print(\"\\n===== INT8 Model (Quantized) =====\")\n",
    "    print_metrics(\n",
    "        int8_metrics, \n",
    "        model_name=f\"INT8 Quantized Student\", \n",
    "        dataset_info=f\"Text Classification - {DATASET_NAME}/{DATASET_CONFIG}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key metrics for comparison with baseline\n",
    "models = ['Teacher', 'Student', 'FP16']\n",
    "metrics_list = [baseline_metrics, student_metrics, fp16_metrics]\n",
    "\n",
    "if int8_metrics is not None:\n",
    "    models.append('INT8')\n",
    "    metrics_list.append(int8_metrics)\n",
    "\n",
    "# Extract metrics\n",
    "sizes = [m['model_size_mb'] for m in metrics_list]\n",
    "latencies = [m['avg_latency_ms'] for m in metrics_list]\n",
    "accuracies = [m['accuracy'] * 100 for m in metrics_list]\n",
    "params = [m['num_parameters'] / 1e6 for m in metrics_list]\n",
    "\n",
    "# Calculate reductions\n",
    "size_reductions = [(1 - size/sizes[0]) * 100 for size in sizes]\n",
    "speed_improvements = [latencies[0]/latency for latency in latencies]\n",
    "accuracy_retentions = [acc/accuracies[0] for acc in accuracies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Size (MB)': [f\"{size:.2f}\" for size in sizes],\n",
    "    'Size Reduction (%)': [f\"{reduction:.2f}\" for reduction in size_reductions],\n",
    "    'Accuracy (%)': [f\"{acc:.2f}\" for acc in accuracies],\n",
    "    'Accuracy Retention (%)': [f\"{retention*100:.2f}\" for retention in accuracy_retentions],\n",
    "    'Latency (ms)': [f\"{latency:.2f}\" for latency in latencies],\n",
    "    'Speed Improvement (x)': [f\"{improvement:.2f}\" for improvement in speed_improvements],\n",
    "    'Parameters (M)': [f\"{param:.2f}\" for param in params]\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart comparisons\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Size comparison\n",
    "axs[0, 0].bar(models, sizes)\n",
    "axs[0, 0].set_title('Model Size (MB)')\n",
    "axs[0, 0].set_ylabel('Size (MB)')\n",
    "axs[0, 0].grid(axis='y')\n",
    "for i, v in enumerate(sizes):\n",
    "    axs[0, 0].text(i, v + 2, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "# Latency comparison\n",
    "axs[0, 1].bar(models, latencies)\n",
    "axs[0, 1].set_title('Inference Latency (ms)')\n",
    "axs[0, 1].set_ylabel('Latency (ms)')\n",
    "axs[0, 1].grid(axis='y')\n",
    "for i, v in enumerate(latencies):\n",
    "    axs[0, 1].text(i, v + 2, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "# Accuracy comparison\n",
    "axs[1, 0].bar(models, accuracies)\n",
    "axs[1, 0].set_title('Accuracy (%)')\n",
    "axs[1, 0].set_ylabel('Accuracy (%)')\n",
    "axs[1, 0].grid(axis='y')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axs[1, 0].text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "# Size reduction comparison\n",
    "axs[1, 1].bar(models, size_reductions)\n",
    "axs[1, 1].set_title('Size Reduction (%)')\n",
    "axs[1, 1].set_ylabel('Reduction (%)')\n",
    "axs[1, 1].grid(axis='y')\n",
    "axs[1, 1].axhline(y=50, color='r', linestyle='--', label='50% Target')\n",
    "axs[1, 1].legend()\n",
    "for i, v in enumerate(size_reductions):\n",
    "    axs[1, 1].text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'model_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Best Quantized Model\n",
    "\n",
    "Save the best performing quantized model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FP16 model (typically the best balance of size and performance)\n",
    "fp16_save_path = QUANTIZED_DIR / \"fp16\"\n",
    "fp16_save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "fp16_model.save_pretrained(fp16_save_path)\n",
    "tokenizer.save_pretrained(fp16_save_path)\n",
    "\n",
    "print(f\"FP16 quantized model saved to {fp16_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for deployment (optional)\n",
    "try:\n",
    "    print(\"Exporting to ONNX format...\")\n",
    "    onnx_path = QUANTIZED_DIR / \"model.onnx\"\n",
    "    onnx_file, onnx_size = export_to_onnx(fp16_model, tokenizer, onnx_path)\n",
    "    print(f\"ONNX model exported to {onnx_file} (Size: {onnx_size:.2f} MB)\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Conclusion\n",
    "\n",
    "Review the optimization results and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model's metrics\n",
    "best_metrics = fp16_metrics if fp16_metrics['accuracy'] >= (int8_metrics['accuracy'] if int8_metrics else 0) else int8_metrics\n",
    "best_model_name = \"FP16\" if best_metrics is fp16_metrics else \"INT8\"\n",
    "\n",
    "# Calculate final metrics\n",
    "final_size_reduction = (1 - best_metrics['model_size_mb'] / baseline_metrics['model_size_mb']) * 100\n",
    "final_speed_improvement = baseline_metrics['avg_latency_seconds'] / best_metrics['avg_latency_seconds']\n",
    "final_accuracy_retention = best_metrics['accuracy'] / baseline_metrics['accuracy'] * 100\n",
    "\n",
    "print(\"===== LLM Optimization Summary =====\")\n",
    "print(f\"\\nStarting point (Teacher model):\")\n",
    "print(f\"  - Size: {baseline_metrics['model_size_mb']:.2f} MB\")\n",
    "print(f\"  - Parameters: {baseline_metrics['num_parameters']:,}\")\n",
    "print(f\"  - Accuracy: {baseline_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  - Inference time: {baseline_metrics['avg_latency_ms']:.2f} ms\")\n",
    "\n",
    "print(f\"\\nFinal optimized model (Distilled + {best_model_name} Quantization):\")\n",
    "print(f\"  - Size: {best_metrics['model_size_mb']:.2f} MB\")\n",
    "print(f\"  - Parameters: {best_metrics['num_parameters']:,}\")\n",
    "print(f\"  - Accuracy: {best_metrics['accuracy']*100:.2f}%\")\n",
    "print(f\"  - Inference time: {best_metrics['avg_latency_ms']:.2f} ms\")\n",
    "\n",
    "print(f\"\\nOptimization results:\")\n",
    "print(f\"  - Size reduction: {final_size_reduction:.2f}%\")\n",
    "print(f\"  - Speed improvement: {final_speed_improvement:.2f}x\")\n",
    "print(f\"  - Accuracy retention: {final_accuracy_retention:.2f}%\")\n",
    "\n",
    "print(\"\\nOptimization targets:\")\n",
    "print(f\"  - Size target (50% reduction): {'✅ Success' if final_size_reduction >= 50 else '❌ Not reached'}\")\n",
    "print(f\"  - Accuracy target (90% retention): {'✅ Success' if final_accuracy_retention >= 90 else '❌ Not reached'}\")\n",
    "print(f\"  - Overall result: {'✅ All targets achieved!' if final_size_reduction >= 50 and final_accuracy_retention >= 90 else '❌ Some targets not reached'}\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "if final_size_reduction >= 50 and final_accuracy_retention >= 90:\n",
    "    print(\"The combination of knowledge distillation and quantization successfully optimized the model\")\n",
    "    print(f\"for edge deployment, achieving both our size reduction and accuracy retention targets.\")\n",
    "elif final_size_reduction >= 50:\n",
    "    print(\"While we achieved our size reduction target, accuracy was impacted more than expected.\")\n",
    "    print(\"Consider fine-tuning the quantized model or exploring other optimization approaches.\")\n",
    "elif final_accuracy_retention >= 90:\n",
    "    print(\"While accuracy remained high, we didn't quite reach our size reduction target.\")\n",
    "    print(\"Consider additional optimization techniques like pruning or further reducing the model architecture.\")\n",
    "else:\n",
    "    print(\"Neither target was fully achieved. Consider revisiting the optimization approach\")\n",
    "    print(\"with different hyperparameters or techniques.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}